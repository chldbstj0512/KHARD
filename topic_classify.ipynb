{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from __future__ import annotations\n",
    "\n",
    "# ===== Standard Library =====\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import argparse\n",
    "from typing import List, Tuple, Dict, Iterable\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "# ===== Third-Party =====\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# Text / Features\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    adjusted_rand_score,\n",
    "    homogeneity_score,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/ys0660/2507Sub/KARD/fortopicmodeling_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  index1                                            context  \\\n",
      "0           0   16191                           혹시나 했는데 역시 헌금때문에 하는게 맞군요   \n",
      "1           1   24855                              배추앓이님 얼마나 당황스럽겠읍니까 ㅋㅋ   \n",
      "2           2   20864                      작년 연말에 기독교랑 이슬람이랑 막 총질해대지 않았나   \n",
      "3           3   11884  꼬우면 니들도 짱깨 짱꼴라 짱퀴벌레 우한폐렴 소년단 만들어라 ㅋ 너넨 지옥의 짱깨 ...   \n",
      "4           4   10915  근데 저새끼는 저러고 다시 원상복귀함 속좁고 옹졸한게 맨날 지 여친한테도 뱀눈까리뜨...   \n",
      "\n",
      "    Label   혐오 대상 혐오대상(세부)  \n",
      "0      정상     NaN      NaN  \n",
      "1      정상     NaN      NaN  \n",
      "2      정상     NaN      NaN  \n",
      "3  명백한 혐오  ['인종']   ['중국']  \n",
      "4      욕설     NaN      NaN  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25933"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "라벨 컬럼들: ['기타', '성소수자', '연령', '인종', '장애인', '정치', '젠더', '종교', '지역']\n",
      "   Unnamed: 0  index1                                            context  \\\n",
      "0           0   16191                           혹시나 했는데 역시 헌금때문에 하는게 맞군요   \n",
      "1           1   24855                              배추앓이님 얼마나 당황스럽겠읍니까 ㅋㅋ   \n",
      "2           2   20864                      작년 연말에 기독교랑 이슬람이랑 막 총질해대지 않았나   \n",
      "3           3   11884  꼬우면 니들도 짱깨 짱꼴라 짱퀴벌레 우한폐렴 소년단 만들어라 ㅋ 너넨 지옥의 짱깨 ...   \n",
      "4           4   10915  근데 저새끼는 저러고 다시 원상복귀함 속좁고 옹졸한게 맨날 지 여친한테도 뱀눈까리뜨...   \n",
      "\n",
      "    Label 혐오대상(세부) label  기타  성소수자  연령  인종  장애인  정치  젠더  종교  지역  \n",
      "0      정상      NaN    []   0     0   0   0    0   0   0   0   0  \n",
      "1      정상      NaN    []   0     0   0   0    0   0   0   0   0  \n",
      "2      정상      NaN    []   0     0   0   0    0   0   0   0   0  \n",
      "3  명백한 혐오   ['중국']  [인종]   0     0   0   1    0   0   0   0   0  \n",
      "4      욕설      NaN    []   0     0   0   0    0   0   0   0   0  \n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def parse_multi_labels(x):\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            val = ast.literal_eval(x) \n",
    "            val = [str(v).strip() for v in val]\n",
    "            val = [v for v in val if v]\n",
    "            return val\n",
    "        except Exception:\n",
    "            return [x.strip()]\n",
    "    return [str(x).strip()]\n",
    "\n",
    "# 1) 파싱\n",
    "df[\"label\"] = df[\"혐오 대상\"].apply(parse_multi_labels)\n",
    "\n",
    "# 2) 멀티-원핫 변환\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(df[\"label\"])   # shape: (N, C)\n",
    "\n",
    "# 3) DataFrame으로 붙이기 (boolean 또는 0/1)\n",
    "label_cols = mlb.classes_.tolist()\n",
    "df_onehot = pd.DataFrame(Y, columns=label_cols, index=df.index).astype(np.uint8)\n",
    "\n",
    "# 4) 원본에 합치기\n",
    "df = pd.concat([df.drop(columns=[\"혐오 대상\"]), df_onehot], axis=1)\n",
    "\n",
    "print(\"라벨 컬럼들:\", label_cols)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25933"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### openAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] Call API for Embeddings...\n"
     ]
    }
   ],
   "source": [
    "# api call\n",
    "n_clusters = 9\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "all_vecs = []\n",
    "batch_size = 128  # 배치 크기 조정 가능\n",
    "model_name = \"text-embedding-3-small\"\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "MAX_TOKENS = 8192\n",
    "\n",
    "def truncate_text(text, max_tokens=MAX_TOKENS):\n",
    "    tokens = enc.encode(text)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "    return enc.decode(tokens)\n",
    "\n",
    "print(\"[2] Call API for Embeddings...\")\n",
    "\n",
    "def get_embed_dim(model_name: str) -> int:\n",
    "    name = model_name.lower()\n",
    "    if \"text-embedding-3-large\" in name:\n",
    "        return 3072\n",
    "    # small/ada-002 등 기본 1536\n",
    "    return 1536\n",
    "\n",
    "\n",
    "def embed_texts_and_align_labels(\n",
    "    texts: List[str],\n",
    "    labels: List[int],\n",
    "    model: str = \"text-embedding-3-small\"\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, Dict[str, List[int]]]:\n",
    "    assert len(texts) == len(labels), f\"len(texts)={len(texts)} vs len(labels)={len(labels)}\"\n",
    "    N = len(texts)\n",
    "    D = get_embed_dim(model)\n",
    "\n",
    "    X = np.zeros((N, D), dtype=np.float32)\n",
    "    mask_nonzero = np.zeros(N, dtype=bool)\n",
    "\n",
    "    ok_idx: List[int] = []\n",
    "    empty_idx: List[int] = []\n",
    "    fail_idx: List[int] = []\n",
    "\n",
    "    for idx, raw in enumerate(tqdm(texts, desc=\"Embedding\", total=N)):\n",
    "        text = truncate_text(raw)\n",
    "        if not text:\n",
    "            empty_idx.append(idx)\n",
    "            continue\n",
    "        try:\n",
    "            resp = client.embeddings.create(model=model, input=text)\n",
    "            vec = np.asarray(resp.data[0].embedding, dtype=np.float32)\n",
    "\n",
    "            if vec.shape[0] != D:\n",
    "                print(f\"[warn] idx={idx} dim change: {vec.shape[0]} != {D}. Adjusting matrix.\")\n",
    "                newD = vec.shape[0]\n",
    "                if newD > D:\n",
    "                    pad = np.zeros((N, newD - D), dtype=np.float32)\n",
    "                    X = np.hstack([X, pad])  # 오른쪽에 0 패딩\n",
    "                else:\n",
    "                    X = X[:, :newD]\n",
    "                D = newD\n",
    "\n",
    "            X[idx] = vec\n",
    "            mask_nonzero[idx] = True\n",
    "            ok_idx.append(idx)\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] idx={idx} embedding failed: {e!r}\")\n",
    "            fail_idx.append(idx)\n",
    "\n",
    "    # 요약 로그\n",
    "    print(\"\\n=== Embedding Summary ===\")\n",
    "    print(f\"Total inputs   : {N}\")\n",
    "    print(f\"OK             : {len(ok_idx)}\")\n",
    "    print(f\"Empty texts    : {len(empty_idx)}\")\n",
    "    print(f\"API failures   : {len(fail_idx)}\")\n",
    "    print(f\"Kept (non-zero): {mask_nonzero.sum()} | Dropped: {(~mask_nonzero).sum()}\")\n",
    "    if empty_idx[:5]:\n",
    "        print(f\"First empty idx: {empty_idx[:5]}\")\n",
    "    if fail_idx[:5]:\n",
    "        print(f\"First fail idx : {fail_idx[:5]}\")\n",
    "\n",
    "    # 같은 마스크로 라벨/텍스트 필터링\n",
    "    labels_arr = np.asarray(labels)\n",
    "    texts_arr  = np.asarray(texts, dtype=object)\n",
    "    labels_clean = labels_arr[mask_nonzero]\n",
    "    texts_clean  = texts_arr[mask_nonzero]\n",
    "\n",
    "    logs = {\"ok\": ok_idx, \"empty\": empty_idx, \"fail\": fail_idx, \"dim\": D}\n",
    "    return X, mask_nonzero, labels_clean, texts_clean, logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 25933/25933 [2:45:53<00:00,  2.61it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Embedding Summary ===\n",
      "Total inputs   : 25933\n",
      "OK             : 25933\n",
      "Empty texts    : 0\n",
      "API failures   : 0\n",
      "Kept (non-zero): 25933 | Dropped: 0\n",
      "Before/After: (25933, 1536) -> (25933, 1536)\n",
      "[saved] /home/ys0660/2507Sub/KARD/data.npz | X=(25933, 1536), labels=(25933, 9), texts=(25933,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def save_clean_pack(save_path: str, X_clean: np.ndarray,\n",
    "                    labels_clean: np.ndarray, texts_clean: np.ndarray):\n",
    "    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    np.savez(save_path, X=X_clean, labels=labels_clean, texts=texts_clean)\n",
    "    print(f\"[saved] {save_path} | X={X_clean.shape}, labels={labels_clean.shape}, texts={texts_clean.shape}\")\n",
    "\n",
    "# 예시: df_clean에 다음 컬럼이 원핫 라벨이라고 가정\n",
    "label_cols = ['기타','성소수자','연령','인종','장애인','정치','젠더','종교','지역']  # 실제 보유 컬럼명으로 교체\n",
    "\n",
    "texts = df[\"context\"].astype(str).tolist()   # 텍스트는 리스트로\n",
    "y_true = df[label_cols].astype(\"uint8\").values  # (N, C) 멀티-원핫 행렬\n",
    "\n",
    "# 실제 실행\n",
    "X, mask_nonzero, labels_clean, texts_clean, logs = embed_texts_and_align_labels(\n",
    "    texts, y_true, model=\"text-embedding-3-small\"\n",
    ")\n",
    "X_clean = X[mask_nonzero]  # 여기서만 마스크 적용\n",
    "\n",
    "print(\"Before/After:\", X.shape, \"->\", X_clean.shape)\n",
    "\n",
    "# 저장\n",
    "save_path = \"/home/ys0660/2507Sub/KARD/data.npz\"\n",
    "save_clean_pack(save_path, X_clean, labels_clean, texts_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/home/ys0660/2507Sub/KARD/data.npz\"\n",
    "save_clean_pack(save_path, X, labels_clean, texts_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### npz load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (25933, 1536) (25933, 9) (25933,)\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/home/ys0660/2507Sub/KARD/data.npz\"\n",
    "\n",
    "# 불러오기\n",
    "data = np.load(save_path, allow_pickle=True)\n",
    "X, y_true, texts = data[\"X\"], data[\"labels\"], data[\"texts\"]\n",
    "print(\"Loaded:\", X.shape, y_true.shape, texts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = [\"명백한 혐오\", \"맥락적 혐오\", \"모호한 혐오\"]\n",
    "case1_df = df[df[\"Label\"].isin(target_labels)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case1 고유값 수: 10432\n",
      "교집합 수(= 뽑힌 인덱스 수): 10432\n",
      "예시 인덱스 몇 개: [3, 5, 9, 12, 14, 15, 17, 21, 25, 30]\n",
      "매칭된 대표 인덱스 수: 10432\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)   # 여러 공백 → 1칸\n",
    "    return s.strip().lower()\n",
    "\n",
    "# 1) 정규화\n",
    "case1_df_unique = case1_df.drop_duplicates(subset=[\"context\"]).copy()\n",
    "case1_norm = case1_df_unique[\"context\"].map(norm).values\n",
    "texts_norm = np.array([norm(t) for t in texts])\n",
    "\n",
    "# 2) texts의 \"값 → 첫 인덱스\" 사전 만들기 (대표 인덱스)\n",
    "first_idx = {}\n",
    "for i, v in enumerate(texts_norm):\n",
    "    if v not in first_idx:\n",
    "        first_idx[v] = i\n",
    "\n",
    "# 3) case1 쪽 고유값에 대해 교집합만 첫 인덱스 뽑기 (값당 1개)\n",
    "targets = pd.unique(case1_norm)\n",
    "case1_first_idx = [first_idx[v] for v in targets if v in first_idx]\n",
    "\n",
    "print(\"case1 고유값 수:\", len(targets))\n",
    "print(\"교집합 수(= 뽑힌 인덱스 수):\", len(case1_first_idx))\n",
    "print(\"예시 인덱스 몇 개:\", case1_first_idx[:10])\n",
    "\n",
    "# texts를 DF로 만들고 정규화/대표 인덱스 확보\n",
    "texts_df = pd.DataFrame({\"context\": texts})\n",
    "texts_df[\"key\"] = texts_df[\"context\"].map(norm)\n",
    "texts_df[\"orig_idx\"] = np.arange(len(texts_df))\n",
    "\n",
    "# 같은 key가 여러 번 있으면 첫 번째(대표)만 유지\n",
    "texts_df_first = texts_df.drop_duplicates(subset=[\"key\"], keep=\"first\")\n",
    "\n",
    "# case1도 정규화/고유화\n",
    "case1_df_u = case1_df.drop_duplicates(subset=[\"context\"]).copy()\n",
    "case1_df_u[\"key\"] = case1_df_u[\"context\"].map(norm)\n",
    "\n",
    "# 1:1 merge → texts의 대표 인덱스만 들어옴\n",
    "m = case1_df_u.merge(texts_df_first[[\"key\",\"orig_idx\"]], on=\"key\", how=\"inner\")\n",
    "\n",
    "case1_first_idx = m[\"orig_idx\"].to_numpy()\n",
    "print(\"매칭된 대표 인덱스 수:\", len(case1_first_idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### case2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_labels = [\"정상\", \"맥락적 정상\"]\n",
    "case2_df = df[df[\"Label\"].isin(target_labels)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case2 고유값 수: 10222\n",
      "교집합 수(= 뽑힌 인덱스 수): 10222\n",
      "예시 인덱스 몇 개: [0, 1, 2, 6, 7, 10, 11, 13, 16, 19]\n",
      "매칭된 대표 인덱스 수: 10222\n"
     ]
    }
   ],
   "source": [
    "case2_df_unique = case2_df.drop_duplicates(subset=[\"context\"]).copy()\n",
    "case2_norm = case2_df_unique[\"context\"].map(norm).values\n",
    "texts_norm = np.array([norm(t) for t in texts])\n",
    "\n",
    "# 2) texts의 \"값 → 첫 인덱스\" 사전 만들기 (대표 인덱스)\n",
    "first_idx = {}\n",
    "for i, v in enumerate(texts_norm):\n",
    "    if v not in first_idx:\n",
    "        first_idx[v] = i\n",
    "\n",
    "# 3) case2 쪽 고유값에 대해 교집합만 첫 인덱스 뽑기 (값당 1개)\n",
    "targets = pd.unique(case2_norm)\n",
    "case2_first_idx = [first_idx[v] for v in targets if v in first_idx]\n",
    "\n",
    "print(\"case2 고유값 수:\", len(targets))\n",
    "print(\"교집합 수(= 뽑힌 인덱스 수):\", len(case2_first_idx))\n",
    "print(\"예시 인덱스 몇 개:\", case2_first_idx[:10])\n",
    "\n",
    "# texts를 DF로 만들고 정규화/대표 인덱스 확보\n",
    "texts_df = pd.DataFrame({\"context\": texts})\n",
    "texts_df[\"key\"] = texts_df[\"context\"].map(norm)\n",
    "texts_df[\"orig_idx\"] = np.arange(len(texts_df))\n",
    "\n",
    "# 같은 key가 여러 번 있으면 첫 번째(대표)만 유지\n",
    "texts_df_first = texts_df.drop_duplicates(subset=[\"key\"], keep=\"first\")\n",
    "\n",
    "# case1도 정규화/고유화\n",
    "case2_df_u = case2_df.drop_duplicates(subset=[\"context\"]).copy()\n",
    "case2_df_u[\"key\"] = case2_df_u[\"context\"].map(norm)\n",
    "\n",
    "# 1:1 merge → texts의 대표 인덱스만 들어옴\n",
    "m = case2_df_u.merge(texts_df_first[[\"key\",\"orig_idx\"]], on=\"key\", how=\"inner\")\n",
    "\n",
    "case2_first_idx = m[\"orig_idx\"].to_numpy()\n",
    "print(\"매칭된 대표 인덱스 수:\", len(case2_first_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 한국어 데이터셋용 \n",
    "import re, math\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Iterable\n",
    "\n",
    "def build_topics_from_clusters(\n",
    "    texts: List[str],\n",
    "    labels_pred: List[int],\n",
    "    topn: int = 10,\n",
    "    min_word_len: int = 2,\n",
    "    include_noise: bool = False,         # -1 라벨 포함 여부\n",
    "    stopwords: Iterable[str] = None,\n",
    "    tokenizer_preference: str = \"auto\",  # \"auto\" | \"kiwi\" | \"okt\" | \"regex\"\n",
    ") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    한국어 데이터셋용 토픽 단어 추출 함수 (c-TF-IDF 기반)\n",
    "    - 형태소 분석기(kiwi/okt)가 있으면 우선 사용, 없으면 정규식으로 대체\n",
    "    - 클러스터별 상위 c-TF-IDF 단어를 반환 (index == cluster_id)\n",
    "    - 존재하지 않는 cluster_id 위치는 []로 채움\n",
    "    \"\"\"\n",
    "\n",
    "    # ----- 0) 준비: stopwords -----\n",
    "    default_stop = {\n",
    "        \"것\",\"수\",\"등\",\"및\",\"더\",\"그리고\",\"그러나\",\"하지만\",\"이\",\"그\",\"저\",\"요\",\n",
    "        \"합니다\",\"하였다\",\"했다\",\"하는\",\"하게\",\"하며\",\"에서\",\"으로\",\"에게\",\"까지\",\n",
    "        \"부터\",\"보다\",\"라고\",\"하면\",\"되다\",\"하다\",\"같다\",\"때문\",\n",
    "        \"중\",\"후\",\"전\",\"또한\",\"그리고\",\"입니다\",\n",
    "        \"새끼\", \"진짜\", \"존나\", \"시발\", \"사람\", \"그냥\", \"생각\"\n",
    "    }\n",
    "    stopset = set(default_stop)\n",
    "    if stopwords:\n",
    "        stopset |= set(stopwords)\n",
    "\n",
    "    # ----- 1) 토크나이저 선택 -----\n",
    "    tok_mode = None\n",
    "    kiwi = None\n",
    "    okt = None\n",
    "\n",
    "    def _try_import_kiwi():\n",
    "        nonlocal kiwi\n",
    "        try:\n",
    "            from kiwipiepy import Kiwi\n",
    "            kiwi = Kiwi()\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    def _try_import_okt():\n",
    "        nonlocal okt\n",
    "        try:\n",
    "            from konlpy.tag import Okt\n",
    "            okt = Okt()\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    if tokenizer_preference == \"kiwi\":\n",
    "        tok_mode = \"kiwi\" if _try_import_kiwi() else \"regex\"\n",
    "    elif tokenizer_preference == \"okt\":\n",
    "        tok_mode = \"okt\" if _try_import_okt() else \"regex\"\n",
    "    elif tokenizer_preference == \"regex\":\n",
    "        tok_mode = \"regex\"\n",
    "    else:  # \"auto\"\n",
    "        if _try_import_kiwi():\n",
    "            tok_mode = \"kiwi\"\n",
    "        elif _try_import_okt():\n",
    "            tok_mode = \"okt\"\n",
    "        else:\n",
    "            tok_mode = \"regex\"\n",
    "\n",
    "    # ----- 2) 토큰화 함수 -----\n",
    "    hangul_alnum = re.compile(r\"[가-힣A-Za-z0-9]+\")\n",
    "    digits_only  = re.compile(r\"^\\d+$\")\n",
    "\n",
    "    def tokenize(doc: str) -> List[str]:\n",
    "        doc = (doc or \"\").strip()\n",
    "        if not doc:\n",
    "            return []\n",
    "\n",
    "        if tok_mode == \"kiwi\":\n",
    "            keep_pos = {\"NNG\", \"NNP\", \"SL\"}  # 필요 시 조정\n",
    "            toks = []\n",
    "            for token, pos, _, _ in kiwi.analyze(doc, top_n=1)[0][0]:\n",
    "                if pos in keep_pos and len(token) >= min_word_len and token not in stopset:\n",
    "                    toks.append(token)\n",
    "            return toks\n",
    "\n",
    "        if tok_mode == \"okt\":\n",
    "            nouns = okt.nouns(doc)\n",
    "            return [w for w in nouns if len(w) >= min_word_len and w not in stopset]\n",
    "\n",
    "        # regex fallback\n",
    "        raw = hangul_alnum.findall(doc)\n",
    "        toks = []\n",
    "        for w in raw:\n",
    "            w = w.lower()\n",
    "            if len(w) < min_word_len:\n",
    "                continue\n",
    "            if digits_only.match(w):\n",
    "                continue\n",
    "            if w in stopset:\n",
    "                continue\n",
    "            toks.append(w)\n",
    "        return toks\n",
    "\n",
    "    # ----- 3) 클러스터 집합/크기 계산 -----\n",
    "    unique_labels = sorted(set(labels_pred))\n",
    "    if not include_noise and -1 in unique_labels:\n",
    "        unique_labels.remove(-1)\n",
    "    if not unique_labels:\n",
    "        return []\n",
    "\n",
    "    max_cid = max([lab for lab in unique_labels if lab >= 0] or [0])\n",
    "    topics: List[List[str]] = [[] for _ in range(max_cid + 1)]\n",
    "\n",
    "    # ----- 4) 입력 길이 검증 -----\n",
    "    assert len(texts) == len(labels_pred), \"texts와 labels_pred 길이가 다릅니다.\"\n",
    "\n",
    "    # ----- 5) cluster_id -> 문서 리스트 수집 -----\n",
    "    cluster_docs: Dict[int, List[str]] = {}\n",
    "    for i, lab in enumerate(labels_pred):\n",
    "        if lab == -1 and not include_noise:\n",
    "            continue\n",
    "        if lab < 0:\n",
    "            continue\n",
    "        cluster_docs.setdefault(lab, []).append(texts[i])\n",
    "\n",
    "    if not cluster_docs:\n",
    "        return topics\n",
    "\n",
    "    # ----- 6) 각 클러스터 토큰화 -----\n",
    "    cluster_tokens: Dict[int, List[str]] = {}\n",
    "    for cid, docs in cluster_docs.items():\n",
    "        toks = []\n",
    "        for doc in docs:\n",
    "            toks.extend(tokenize(doc))\n",
    "        cluster_tokens[cid] = toks\n",
    "\n",
    "    # ----- 7) 어휘 사전 구축 -----\n",
    "    vocab: Dict[str, int] = {}\n",
    "    for toks in cluster_tokens.values():\n",
    "        for w in toks:\n",
    "            if w not in vocab:\n",
    "                vocab[w] = len(vocab)\n",
    "    V = len(vocab)\n",
    "    if V == 0:\n",
    "        return topics\n",
    "\n",
    "    inv_vocab = {idx: w for w, idx in vocab.items()}\n",
    "\n",
    "    # ----- 8) 클러스터-단어 카운트 행렬 (TF) -----\n",
    "    counts = {cid: [0] * V for cid in cluster_tokens.keys()}\n",
    "    total_words_per_class = {}\n",
    "    for cid, toks in cluster_tokens.items():\n",
    "        arr = counts[cid]\n",
    "        for w in toks:\n",
    "            arr[vocab[w]] += 1\n",
    "        total_words_per_class[cid] = sum(arr)\n",
    "\n",
    "    # ----- 9) c-TF-IDF 계수 계산 -----\n",
    "    # tf_t: 모든 클러스터에서의 term 총 빈도\n",
    "    tf_t = [0] * V\n",
    "    for arr in counts.values():\n",
    "        for j in range(V):\n",
    "            tf_t[j] += arr[j]\n",
    "\n",
    "    C = len(cluster_tokens)  # 비어있지 않은 클러스터 수\n",
    "    A = sum(total_words_per_class.values()) / float(C)  # 클래스별 평균 단어 수\n",
    "\n",
    "    # inv_class_part[j] = log(1 + A / tf_t[j])\n",
    "    inv_class_part = [0.0] * V\n",
    "    for j in range(V):\n",
    "        inv_class_part[j] = math.log(1.0 + (A / tf_t[j])) if tf_t[j] > 0 else 0.0\n",
    "\n",
    "    # ----- 10) c-TF-IDF 상위 단어 추출 -----\n",
    "    for cid in range(max_cid + 1):\n",
    "        if cid not in cluster_tokens:\n",
    "            topics[cid] = []\n",
    "            continue\n",
    "\n",
    "        arr = counts[cid]\n",
    "        # W_{t,c} = tf_{t,c} * log(1 + A / tf_t)\n",
    "        scores = [(j, arr[j] * inv_class_part[j]) for j in range(V) if arr[j] > 0]\n",
    "        if not scores:\n",
    "            topics[cid] = []\n",
    "            continue\n",
    "\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_items = scores[:topn]\n",
    "        topics[cid] = [inv_vocab[j] for j, _ in top_items]\n",
    "\n",
    "    return topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "### K-means++\n",
    "def cluster_kmeans(X: np.ndarray, n_clusters: int, seed: int = 42) -> np.ndarray:\n",
    "    km = KMeans(\n",
    "        n_clusters=n_clusters, \n",
    "        init=\"k-means++\", \n",
    "        n_init=30,\n",
    "        random_state=seed  \n",
    "    )\n",
    "    labels = km.fit_predict(X)\n",
    "    return labels\n",
    "\n",
    "def hungarian_match(labels_true: List[int], labels_pred: List[int]) -> Tuple[Dict[int, int], np.ndarray]:\n",
    "    \"\"\"\n",
    "    예측 클러스터 라벨(labels_pred)을 실제 라벨(labels_true)에 최대 일치하도록 재매핑.\n",
    "    반환: (매핑 딕셔너리, 재매핑된 예측 라벨 배열)\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(labels_true)\n",
    "    y_pred = np.asarray(labels_pred)\n",
    "\n",
    "    # 라벨들을 0..K-1로 압축(불연속 라벨 대비)\n",
    "    true_ids, y_true_comp = np.unique(y_true, return_inverse=True)\n",
    "    pred_ids, y_pred_comp = np.unique(y_pred, return_inverse=True)\n",
    "\n",
    "    K_true = true_ids.size\n",
    "    K_pred = pred_ids.size\n",
    "    K = max(K_true, K_pred)\n",
    "\n",
    "    # 혼동행렬 (사이즈를 같게 맞추기 위해 패딩)\n",
    "    cm = confusion_matrix(y_true_comp, y_pred_comp, labels=np.arange(max(K_true, K_pred)))\n",
    "    if cm.shape[0] < K or cm.shape[1] < K:\n",
    "        pad_r = K - cm.shape[0]\n",
    "        pad_c = K - cm.shape[1]\n",
    "        cm = np.pad(cm, ((0, pad_r), (0, pad_c)), mode=\"constant\", constant_values=0)\n",
    "\n",
    "    # 비용 행렬 = -cm (최대 매칭을 최소 비용으로)\n",
    "    cost = -cm\n",
    "    row_ind, col_ind = linear_sum_assignment(cost)\n",
    "\n",
    "    # pred 라벨(압축 기준) -> true 라벨(원본 기준) 매핑 구성\n",
    "    mapping_comp = {int(col): int(row) for row, col in zip(row_ind, col_ind)}\n",
    "    # 압축 라벨을 원래 라벨 값으로 복원\n",
    "    mapping = {}\n",
    "    for pred_comp, true_comp in mapping_comp.items():\n",
    "        if pred_comp < K_pred and true_comp < K_true:\n",
    "            mapping[int(pred_ids[pred_comp])] = int(true_ids[true_comp])\n",
    "\n",
    "    # 재매핑 적용\n",
    "    y_pred_aligned = np.array([mapping.get(lbl, lbl) for lbl in y_pred], dtype=int)\n",
    "    return mapping, y_pred_aligned\n",
    "\n",
    "\n",
    "def class_hungarian_metrics(metrics: dict, label_names=None, save_prefix: str | None = None):\n",
    "    \"\"\"\n",
    "    metrics: evaluate_with_hungarian_class(...) 반환 dict\n",
    "    label_names: {int_label: \"name\"} 형태 선택 제공\n",
    "    save_prefix: \"runs/exp1\" 처럼 주면 CSV/JSON 저장\n",
    "    \"\"\"\n",
    "    # ---- 요약 ----\n",
    "    acc = metrics.get(\"accuracy\")\n",
    "    f1w = metrics.get(\"f1_weighted\")\n",
    "    ari = metrics.get(\"ari\")\n",
    "    hs  = metrics.get(\"homogeneity\")\n",
    "    print(\"=== Summary ===\")\n",
    "    print(f\"ACC={acc:.4f} | F1(w)={f1w:.4f} | ARI={ari:.4f} | HS={hs:.4f}\")\n",
    "\n",
    "    # ---- 매핑 표 ----\n",
    "    mapping = metrics.get(\"mapping\", {})\n",
    "    if mapping:\n",
    "        df_map = pd.DataFrame(sorted(mapping.items()), columns=[\"pred_cluster\", \"mapped_label\"])\n",
    "        if label_names:\n",
    "            df_map[\"mapped_name\"] = df_map[\"mapped_label\"].map(lambda x: label_names.get(x, x))\n",
    "        print(\"\\n=== Hungarian Mapping (pred -> true) ===\")\n",
    "        print(df_map.to_string(index=False))\n",
    "\n",
    "    # ---- 클래스별 리포트 ----\n",
    "    report = metrics.get(\"classification_report\", {})\n",
    "    # classification_report dict에는 'accuracy','macro avg','weighted avg'도 섞여 있으므로 숫자 키만 선택\n",
    "    per_class = {k: v for k, v in report.items() if k.isdigit()}\n",
    "    df_cls = pd.DataFrame(per_class).T\n",
    "    # 타입 정리 및 정렬\n",
    "    for col in [\"precision\", \"recall\", \"f1-score\", \"support\"]:\n",
    "        if col in df_cls:\n",
    "            df_cls[col] = pd.to_numeric(df_cls[col], errors=\"coerce\")\n",
    "    df_cls.index = df_cls.index.astype(int)\n",
    "    if label_names:\n",
    "        df_cls.insert(0, \"name\", df_cls.index.map(lambda x: label_names.get(x, x)))\n",
    "    df_cls = df_cls.sort_values(\"support\", ascending=False)\n",
    "\n",
    "    print(\"\\n=== Per-Class (sorted by support) ===\")\n",
    "    # 소수점 보기 좋게\n",
    "    display_cols = [c for c in [\"name\",\"precision\",\"recall\",\"f1-score\",\"support\"] if c in df_cls.columns]\n",
    "    print(df_cls[display_cols].round(4).to_string())\n",
    "\n",
    "    # ---- 최고/최저 리콜 TOP5 ----\n",
    "    if \"recall\" in df_cls:\n",
    "        worst5 = df_cls.nsmallest(5, \"recall\")\n",
    "        best5  = df_cls.nlargest(5, \"recall\")\n",
    "        print(\"\\n=== Lowest Recall TOP5 ===\")\n",
    "        print(worst5[display_cols].round(4).to_string())\n",
    "        print(\"\\n=== Highest Recall TOP5 ===\")\n",
    "        print(best5[display_cols].round(4).to_string())\n",
    "\n",
    "    # ---- 저장 옵션 ----\n",
    "    if save_prefix:\n",
    "        # 원본 dict 저장\n",
    "        with open(f\"{save_prefix}_metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "        # 표 저장\n",
    "        df_map.to_csv(f\"{save_prefix}_mapping.csv\", index=False)\n",
    "        df_cls.to_csv(f\"{save_prefix}_per_class.csv\")\n",
    "        print(f\"\\n[saved] {save_prefix}_metrics.json / {save_prefix}_mapping.csv / {save_prefix}_per_class.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_case1 = X[case1_first_idx]\n",
    "texts_case1 = texts[case1_first_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_case2 = X[case2_first_idx]\n",
    "texts_case2 = texts[case2_first_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 30\n",
    "topn = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = cluster_kmeans(X_case2, n_clusters=n_clusters, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "case2_df_unique[\"cluster30\"] = y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['기온', '광주', '전라도', '전주', '바람', '지역', '대구', '전국', '서울', '부산', '지방', '경상도', '다소', '주차', '해상', '지하철', '동남아', '제주', '축제', '기타', '보통', '기질', '하늘', '사투리', '남해', '양호', '예상', '군산', '전날', '내륙']\n",
      "Topic 1: ['양형모', '머리', '굴맨', '냄새', '염색', '어그', '느낌', '클릭', '감염', '이미지', '끼리', '얼굴', '보기', '가슴', '제일', '악마', '보틀', '정도', '처리', '오지', '징징', '히든', '똥파리', '자체', '조각', '색상', '콘돔', '접대', '설정', '똥꼬']\n",
      "Topic 2: ['치카', '이슬람', '클라스', '무슬림', '게이', '영화', '리버풀', '추천', '코스프레', '호프', '클럽', '테러리스트', '선수', '캐릭터', '린더', '엘리엇', '신부', '레알', '발매', '마블', '히어로', '테러', '공개', '에브라', '미코', '레이드', '러시아', '피터', '토씨', '스타']\n",
      "Topic 3: ['문학', '인간', '과학', '존재', '이해', '논리', '자기', '자체', '사실', '문제', '정의', '그게', '지능', '대해', '앨빈', '정신', '폭력', '지식', '정신병', '하나님', '충격', '의미', '가지', '공부', '창조', '사회', '언어', '가치', '축제', '니체']\n",
      "Topic 4: ['단어', '소리', '똥꼬', '영어', '좌빨', '표현', '짱깨', '개독', '얘기', '일베', '용어', '게이', '헛소리', '혐오', '자체', '의미', '일베충', '무슨', '사투리', '이해', '발음', '말로', '언어', '남충', '하나', '문자', '정도', '듣기', '이란', '우리']\n",
      "Topic 5: ['댓글', '기사', '게시판', '금지', '사이트', '뉴스', '게시', '카페', '일베', '오유', '삭제', '방송', '회원', '커뮤니티', '활동', '인터넷', '단어', '루리웹', '일러스트', '추천', '덧글', '정지', '회사', '하나', '네이버', '디씨', '가입', '드립', '멤버', '유저']\n",
      "Topic 6: ['노인', '노답', '노숙자', '노래', '염전', '노예', '노인학', '노동자', '고립', '공경', '급식', '노노', '사투리', '사회', '신안군', '소수자', '경상도', '노약', '거리', '귀족', '노무현', '얼굴', '일베', '황교익', '얼마나', '노력', '자석', '넷상', '버젼', '자리']\n",
      "Topic 7: ['주문', '수고', '밴드', '선물', '준비', '추천', '예약', '감사', '응원', '미남', '오늘', '제발', '시간', '신고', '축하', '토요일', '짱구', '여러분', '완료', '걷기', '독거', '스크린', '배송', '기도', '정보', '이번', '대감', '당신', '구라', '발송']\n",
      "Topic 8: ['채용', '장애인', '공고', '첨부파일', '링크', '근무', '주차', '지역', '등록', '근로자', '대구광역시', '장애인체육', '마감', '직급', '전형', '경찰', '기관', '기간', '지원', '신천지', '관리', '코로나', '제목', '접수', '조사', '삭감', '생활', '서류', '시설', '서울']\n",
      "Topic 9: ['대통령', '후보', '민주당', '국민', '문재인', '보수', '정치', '대표', '진보', '이재명', '의원', '언론', '정부', '윤석열', '대선', '문제', '정권', '지지', '대한', '성소수자', '박근혜', '야당', '이준석', '지지자', '발언', '대해', '개혁', '장관', '선거', '비판']\n",
      "Topic 10: ['게임', '패턴', '포도', '공격', '소환', '그림', '데미지', '시간', '개미', '정도', '플레이', '도깨비', '캐릭터', '보스', '택포', '벌레', '영화', '스킬', '주포', '속도', '유충', '시작', '티어', '게이', '등장', '에일리언', '수정', '매혹', '패치', '번개']\n",
      "Topic 11: ['아버지', '엄마', '아들', '할머니', '아이', '할아버지', '꼬치', '어머니', '부모', '몸통', '아빠', '보고', '부모님', '머리', '우리', '바보', '정도', '노인', '친구', '거부', '준용', '자식', '서울', '하숙', '소리', '평가', '애기', '아줌마', '삼촌', '성격']\n",
      "Topic 12: ['김정은', '저기', '사진', '가지가지', '선동', '아뇨', '실패', '무슬림', '주무', '자장면', '란데', '패왕색', '카연', '쭈차뿔', '미먀', '호크룩스', '뭐짘', '글쿠먼', '겻다', '보노', '신우진', '라용', '브럽다', '유혜연', '패션쇼', '발꼬락', '인감', '업댐', '렬루', '참고']\n",
      "Topic 13: ['남자', '여자', '여성', '남성', '남녀', '사회', '결혼', '혐오', '대한', '게이', '재희', '단어', '소설', '반대', '찬성', '문제', '차별', '박상영', '일반', '사랑', '성차별', '평등', '갈등', '관계', '연애', '여가부', '우리', '자신', '남충', '주장']\n",
      "Topic 14: ['문제', '광고', '군대', '다행', '일단', '자꾸', '안함', '건가', '무조건', '비번', '계속', '그닥', '당분간', '변명', '아무', '아예', '가능', '무시', '손가락', '필요', '치면', '하나', '알고리즘', '그거', '운전', '절대', '다시', '피해', '스토리', '기복']\n",
      "Topic 15: ['중국', '중국인', '한국', '미국', '짱깨', '조선족', '한국인', '중국어', '아우디', '대만', '홍콩', '외국인', '사형', '우리나라', '제품', '일본인', '일본', '중국집', '관광객', '귀화', '선수', '비트코인', '판다', '우리', '동포', '북한', '중국산', '너구리', '한족', '나라']\n",
      "Topic 16: ['동성애', '동성애자', '성소수자', '이성애자', '반대', '차별', '혐오', '성적', '장애인', '자신', '게이', '개인', '사회', '정신병', '에이즈', '문제', '유전자', '대한', '존중', '찬성', '성정체성', '이유', '단체', '다른', '결혼', '대해', '대다수', '커밍아웃', '취향', '비난']\n",
      "Topic 17: ['처벌', '경찰', '사건', '발언', '대한', '제재', '허위', '검찰', '피해자', '곤충', '혐의', '정치', '사실', '성소수자', '신고', '문제', '본인', '법원', '내용', '사과', '불법', '사회', '책임', '자기', '재판', '이재명', '관련', '대해', '자신', '장애인']\n",
      "Topic 18: ['홍어', '홍어회', '흑산도', '냄새', '후기', '한잔', '숙성', '마트', '막걸리', '이벤트', '홍어삼합', '구입', '오늘', '상품', '작성', '애탕', '주문', '가오리', '생선', '정말', '카페', '수육', '양식', '민어', '어제', '아주', '처음', '사장', '건조', '김치']\n",
      "Topic 19: ['한국', '미국', '국민', '나라', '정부', '일본', '국가', '우리', '외국인', '중국', '조선족', '우리나라', '북한', '정치', '경제', '평화', '사회', '난민', '정책', '다문화', '세계', '문제', '한국인', '더욱', '지원', '올해', '대한민국', '전쟁', '하나', '이라크']\n",
      "Topic 20: ['투표', '보수', '차이', '개표', '지지율', '선거', '이준석', '소스', '경상도', '총선', '조선족', '소수', '진보', '평균', '등판', '지지', '솔로', '정당', '타점', '중도', '대선', '정도', '이번', '사전투표', '수준', '자유당', '사투리', '홍준표', '결집', '폭동']\n",
      "Topic 21: ['페미', '페미니즘', '페미니스트', '여성', '남성', '사회', '운동', '인권', '사상', '대한', '평등', '젠더', '사용', '문제', '단어', '이슈', '지지', '혐오', '남자', '차별', '비판', '권리', '주장', '소리', '의미', '여자', '웅앵웅', '민주당', '아이유', '진보']\n",
      "Topic 22: ['볶음', '호주', '품절', '가격', '주문', '국내', '미역', '택배', '마리', '판매', '반찬', '박스', '배달', '배송', '젓갈', '조림', '새우', '중국', '메뉴', '나물', '멸치', '정도', '연락', '오늘', '석탄', '자연', '고추', '거래', '세금', '수입']\n",
      "Topic 23: ['게로', '다시', '게이', '저건', '우리', '거기', '어케', '어이', '좀해', '누구', '그거', '좌빨', '아줌마', '애가', '그게', '어디', '내일', '이건', '라면', '다음', '홀로', '난리', '요즘', '일베', '더위', '레고', '하나', '거지', '뭔가', '어우']\n",
      "Topic 24: ['유충', '성충', '모기', '잠자리', '장수풍뎅이', '매미', '필터', '곤충', '기생충', '파리', '수돗물', '마리', '벌레', '숙주', '톱밥', '기생', '개체', '똥꼬', '사슴벌레', '유생', '분양', '정수기', '나방', '성체', '나비', '소형', '종류', '입양', '사슴', '갈고리촌충']\n",
      "Topic 25: ['징징', '극혐', '짜장', '도둑', '단체', '은퇴', '직업', '사이비', '프로그램', '하자', '인증', '개월', '주말', '돼지', '대학', '쓰레기', '육갑', '후론트', '김성모', '으잉', '고화질', '곓볡휈', '아시아나', '글치', '박가분', '해악', '워메', '진힐', '염정아', '바밥바']\n",
      "Topic 26: ['친구', '정말', '정도', '보고', '하나', '소리', '시간', '아이', '다시', '라비', '오늘', '빌립', '자기', '마음', '선비', '시작', '장애인', '이제', '학년', '이야기', '인간', '지금', '갑자기', '병원', '하루', '계속', '여우', '여인', '처음', '최충헌']\n",
      "Topic 27: ['교회', '기독교', '종교', '개신교', '목사', '개독', '천주교', '예배', '예수', '불교', '신천지', '하나님', '신도', '천국', '기독교인', '이단', '신앙', '구원', '기도', '사이비', '단체', '종교인', '지옥', '개독교', '교인', '하느님', '성경', '예수님', '자기', '코로나']\n",
      "Topic 28: ['세대', '어른', '나이', '젊은이', '진보', '청년', '대학생', '지금', '남자', '요즘', '살이', '혼인', '투표', '페미', '세기', '지지율', '이상', '민주당', '중반', '꼰대', '경험', '여자', '싸가지', '시대', '대가', '출산율', '옹달샘', '선생', '인터넷', '댓글']\n",
      "Topic 29: ['순대', '김치', '음식', '냉면', '홍어', '고기', '재료', '족발', '주문', '김장', '돼지', '요리', '육수', '반찬', '돼지고기', '간장', '식용', '먹기', '먹음', '소고기', '곤충', '비닐', '전라도', '국물', '국밥', '식당', '마리', '입맛', '함흥', '평양']\n"
     ]
    }
   ],
   "source": [
    "### 상위 단어 추출\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "topics = build_topics_from_clusters(texts_case2, y_pred2, topn=topn)\n",
    "for i, topic in enumerate(topics):\n",
    "    print(f\"Topic {i}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "case2_df_unique.drop(columns=[\"Unnamed: 0\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index1</th>\n",
       "      <th>context</th>\n",
       "      <th>Label</th>\n",
       "      <th>혐오대상(세부)</th>\n",
       "      <th>label</th>\n",
       "      <th>기타</th>\n",
       "      <th>성소수자</th>\n",
       "      <th>연령</th>\n",
       "      <th>인종</th>\n",
       "      <th>장애인</th>\n",
       "      <th>정치</th>\n",
       "      <th>젠더</th>\n",
       "      <th>종교</th>\n",
       "      <th>지역</th>\n",
       "      <th>cluster10</th>\n",
       "      <th>cluster20</th>\n",
       "      <th>cluster30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11884</td>\n",
       "      <td>꼬우면 니들도 짱깨 짱꼴라 짱퀴벌레 우한폐렴 소년단 만들어라 ㅋ 너넨 지옥의 짱깨 ...</td>\n",
       "      <td>명백한 혐오</td>\n",
       "      <td>['중국']</td>\n",
       "      <td>[인종]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14336</td>\n",
       "      <td>이슬람의 발생이나 발전단계를 찾아보면, 유대교가 기독교를 무시하고 천주교도 무시하는...</td>\n",
       "      <td>맥락적 혐오</td>\n",
       "      <td>['이슬람교계']</td>\n",
       "      <td>[종교]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19994</td>\n",
       "      <td>옆에서 쭝얼쭝얼거리면 걍 싫다는듯 시선 피하고 있기만 함 나는 놈들 붙으면 개정색하...</td>\n",
       "      <td>모호한 혐오</td>\n",
       "      <td>['기독교계']</td>\n",
       "      <td>[종교]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15575</td>\n",
       "      <td>헌재에 진보성향 한명 더 넣고 언중법 유튜브와 유사언론까지 확장하면 차금법 이상의 ...</td>\n",
       "      <td>명백한 혐오</td>\n",
       "      <td>['게이관련']</td>\n",
       "      <td>[성소수자]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11424</td>\n",
       "      <td>문제를 제기하면 그건 니 문제다 야훼에게 의문을 가지면 믿음이 모자라서 그렇다 개독...</td>\n",
       "      <td>명백한 혐오</td>\n",
       "      <td>['기독교계']</td>\n",
       "      <td>[종교]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index1                                            context   Label  \\\n",
       "3    11884  꼬우면 니들도 짱깨 짱꼴라 짱퀴벌레 우한폐렴 소년단 만들어라 ㅋ 너넨 지옥의 짱깨 ...  명백한 혐오   \n",
       "5    14336  이슬람의 발생이나 발전단계를 찾아보면, 유대교가 기독교를 무시하고 천주교도 무시하는...  맥락적 혐오   \n",
       "9    19994  옆에서 쭝얼쭝얼거리면 걍 싫다는듯 시선 피하고 있기만 함 나는 놈들 붙으면 개정색하...  모호한 혐오   \n",
       "12   15575  헌재에 진보성향 한명 더 넣고 언중법 유튜브와 유사언론까지 확장하면 차금법 이상의 ...  명백한 혐오   \n",
       "14   11424  문제를 제기하면 그건 니 문제다 야훼에게 의문을 가지면 믿음이 모자라서 그렇다 개독...  명백한 혐오   \n",
       "\n",
       "     혐오대상(세부)   label  기타  성소수자  연령  인종  장애인  정치  젠더  종교  지역  cluster10  \\\n",
       "3      ['중국']    [인종]   0     0   0   1    0   0   0   0   0          2   \n",
       "5   ['이슬람교계']    [종교]   0     0   0   0    0   0   0   1   0          8   \n",
       "9    ['기독교계']    [종교]   0     0   0   0    0   0   0   1   0          0   \n",
       "12   ['게이관련']  [성소수자]   0     1   0   0    0   0   0   0   0          5   \n",
       "14   ['기독교계']    [종교]   0     0   0   0    0   0   0   1   0          0   \n",
       "\n",
       "    cluster20  cluster30  \n",
       "3           9         23  \n",
       "5           8         17  \n",
       "9          16          2  \n",
       "12          4          1  \n",
       "14          4          9  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case1_df_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index1</th>\n",
       "      <th>context</th>\n",
       "      <th>Label</th>\n",
       "      <th>혐오대상(세부)</th>\n",
       "      <th>label</th>\n",
       "      <th>기타</th>\n",
       "      <th>성소수자</th>\n",
       "      <th>연령</th>\n",
       "      <th>인종</th>\n",
       "      <th>장애인</th>\n",
       "      <th>정치</th>\n",
       "      <th>젠더</th>\n",
       "      <th>종교</th>\n",
       "      <th>지역</th>\n",
       "      <th>cluster10</th>\n",
       "      <th>cluster20</th>\n",
       "      <th>cluster30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16191</td>\n",
       "      <td>혹시나 했는데 역시 헌금때문에 하는게 맞군요</td>\n",
       "      <td>정상</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24855</td>\n",
       "      <td>배추앓이님 얼마나 당황스럽겠읍니까 ㅋㅋ</td>\n",
       "      <td>정상</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20864</td>\n",
       "      <td>작년 연말에 기독교랑 이슬람이랑 막 총질해대지 않았나</td>\n",
       "      <td>정상</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12985</td>\n",
       "      <td>비율 와</td>\n",
       "      <td>정상</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22660</td>\n",
       "      <td>이짤 줏었는데 ㅋㅋㅋ 수박타령 똥파리타령 암컷타령 표 주는 사람이 이상하지 ㅋㅋ 그...</td>\n",
       "      <td>정상</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index1                                            context Label 혐오대상(세부)  \\\n",
       "0   16191                           혹시나 했는데 역시 헌금때문에 하는게 맞군요    정상      NaN   \n",
       "1   24855                              배추앓이님 얼마나 당황스럽겠읍니까 ㅋㅋ    정상      NaN   \n",
       "2   20864                      작년 연말에 기독교랑 이슬람이랑 막 총질해대지 않았나    정상      NaN   \n",
       "6   12985                                               비율 와    정상      NaN   \n",
       "7   22660  이짤 줏었는데 ㅋㅋㅋ 수박타령 똥파리타령 암컷타령 표 주는 사람이 이상하지 ㅋㅋ 그...    정상      NaN   \n",
       "\n",
       "  label  기타  성소수자  연령  인종  장애인  정치  젠더  종교  지역  cluster10  cluster20  \\\n",
       "0    []   0     0   0   0    0   0   0   0   0          2         11   \n",
       "1    []   0     0   0   0    0   0   0   0   0          7          5   \n",
       "2    []   0     0   0   0    0   0   0   0   0          4          4   \n",
       "6    []   0     0   0   0    0   0   0   0   0          3         11   \n",
       "7    []   0     0   0   0    0   0   0   0   0          5         17   \n",
       "\n",
       "   cluster30  \n",
       "0         22  \n",
       "1          1  \n",
       "2         27  \n",
       "6         22  \n",
       "7         26  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case2_df_unique.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "case2_df_unique.to_csv('/home/ys0660/2507Sub/KARD/data/case2_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, f1_score, adjusted_rand_score,\n",
    "    homogeneity_score, classification_report, silhouette_score,\n",
    "    calinski_harabasz_score\n",
    ")\n",
    "\n",
    "def compute_npmi(topics, texts, topn=10):\n",
    "    \"\"\"\n",
    "    토픽 coherence (NPMI) 계산용 placeholder 함수.\n",
    "    실제 구현은 Palmetto, Gensim, OCTIS 등의 라이브러리 사용 권장.\n",
    "    \"\"\"\n",
    "    return 0.0\n",
    "\n",
    "def compute_td(topics, topn=10):\n",
    "    \"\"\"\n",
    "    Topic Diversity (TD): 전체 토픽의 상위 단어 중 고유 단어 비율.\n",
    "    topics: List[List[str]] 형태 (각 토픽별 상위 단어 리스트)\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    for t in topics:\n",
    "        all_words.extend(t[:topn])\n",
    "    return len(set(all_words)) / len(all_words) if all_words else 0.0\n",
    "\n",
    "def evaluate_with_hungarian(\n",
    "    labels_true: List[int], \n",
    "    labels_pred: List[int], \n",
    "    X: np.ndarray = None, \n",
    "    topics: List[List[str]] = None, \n",
    "    texts: List[str] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    labels_true: ground truth labels\n",
    "    labels_pred: predicted labels\n",
    "    X: (N, D) feature vectors (SS, CHI용)\n",
    "    topics: topic word lists (topic coherence, diversity용)\n",
    "    texts: raw docs (coherence metric 계산용)\n",
    "    \"\"\"\n",
    "    # Hungarian 매칭 (외부 함수 필요)\n",
    "    mapping, y_pred_aligned = hungarian_match(labels_true, labels_pred)\n",
    "\n",
    "    # External metrics\n",
    "    acc = accuracy_score(labels_true, y_pred_aligned)\n",
    "    recalls = recall_score(labels_true, y_pred_aligned, average=None, zero_division=0)\n",
    "    f1_w = f1_score(labels_true, y_pred_aligned, average=\"weighted\")\n",
    "    ari = adjusted_rand_score(labels_true, labels_pred)\n",
    "    hs  = homogeneity_score(labels_true, labels_pred)\n",
    "    report = classification_report(labels_true, y_pred_aligned, zero_division=0, output_dict=True)\n",
    "\n",
    "    # Internal clustering metrics\n",
    "    ss, chi = None, None\n",
    "    if X is not None and len(set(labels_pred)) > 1:\n",
    "        try:\n",
    "            ss  = silhouette_score(X, labels_pred)\n",
    "            chi = calinski_harabasz_score(X, labels_pred)\n",
    "        except Exception as e:\n",
    "            print(\"[warn] Internal metrics failed:\", e)\n",
    "\n",
    "    # Topic modeling metrics\n",
    "    npmi, td = None, None\n",
    "    if topics is not None:\n",
    "        npmi = compute_npmi(topics, texts)\n",
    "        td   = compute_td(topics)\n",
    "\n",
    "    return {\n",
    "        \"mapping\": mapping,\n",
    "        \"accuracy\": acc,\n",
    "        \"recall_per_class\": recalls.tolist(),\n",
    "        \"f1_weighted\": f1_w,\n",
    "        \"ari\": ari,\n",
    "        \"homogeneity\": hs,\n",
    "        \"classification_report\": report,\n",
    "        \"y_pred_aligned\": y_pred_aligned,\n",
    "        \"silhouette\": ss,\n",
    "        \"calinski_harabasz\": chi,\n",
    "        \"topic_coherence_npmi\": npmi,\n",
    "        \"topic_diversity\": td,\n",
    "    }\n",
    "\n",
    "metrics = evaluate_with_hungarian(y_true, y_pred, X=X, topics=topics, texts=texts)\n",
    "\n",
    "print(\"=== External Metrics (라벨 필요) ===\")\n",
    "print(f\"Accuracy         : {metrics['accuracy']:.4f}\")\n",
    "print(f\"F1-weighted     : {metrics['f1_weighted']:.4f}\")\n",
    "print(f\"ARI             : {metrics['ari']:.4f}\")\n",
    "print(f\"Homogeneity     : {metrics['homogeneity']:.4f}\")\n",
    "\n",
    "print(\"\\n=== Internal Metrics (라벨 없이 군집 품질) ===\")\n",
    "print(f\"Silhouette Score: {metrics['silhouette']}\")\n",
    "print(f\"Calinski-Harabasz Index: {metrics['calinski_harabasz']}\")\n",
    "\n",
    "print(\"\\n=== Topic Modeling Metrics ===\")\n",
    "print(f\"NPMI Coherence  : {metrics['topic_coherence_npmi']}\")\n",
    "print(f\"Topic Diversity : {metrics['topic_diversity']}\")\n",
    "# label 이름이 있으면 더 깔끔\n",
    "label_names = {i: f\"class_{i}\" for i in range(20)}  # 필요 시 실제 이름 매핑\n",
    "metrics = evaluate_with_hungarian(y_true, y_pred, X=X, topics=topics, texts=texts)\n",
    "class_hungarian_metrics(metrics, label_names=label_names, save_prefix=None)\n",
    "### BERTopic Metric\n",
    "# 1) 토큰 코퍼스 만들기 (영어 기준: 소문자/알파벳, 불용어/한 글자 제거)\n",
    "import re\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "\n",
    "STOP = set(ENGLISH_STOP_WORDS) | {\n",
    "    # 수축형/노이즈 보완\n",
    "    \"don\",\"ve\",\"ll\",\"re\",\"im\",\"ms\",\"nd\",\"uw\",\"ww\"\n",
    "}\n",
    "_token_pat = re.compile(r\"[A-Za-z]+\")\n",
    "\n",
    "def corpus_to_tokens(texts, min_len=2):\n",
    "    corpus_tokens = []\n",
    "    for t in texts:\n",
    "        toks = [w.lower() for w in _token_pat.findall(str(t) or \"\")]\n",
    "        toks = [w for w in toks if len(w) >= min_len and w not in STOP]\n",
    "        corpus_tokens.append(toks)\n",
    "    return corpus_tokens\n",
    "\n",
    "corpus_tokens = corpus_to_tokens(texts)  # <-- 주신 numpy array 그대로 전달\n",
    "def evaluate(self, output_tm):\n",
    "        \"\"\"Using metrics and output of the topic model, evaluate the topic model\"\"\"\n",
    "        if self.timestamps:\n",
    "            results = {str(timestamp): {} for timestamp, _ in output_tm.items()}\n",
    "            for timestamp, topics in output_tm.items():\n",
    "                self.metrics = self.get_metrics()\n",
    "                for scorers, _ in self.metrics:\n",
    "                    for scorer, name in scorers:\n",
    "                        score = scorer.score(topics)\n",
    "                        results[str(timestamp)][name] = float(score)\n",
    "\n",
    "        else:\n",
    "            # Calculate results\n",
    "            results = {}\n",
    "            for scorers, _ in self.metrics:\n",
    "                for scorer, name in scorers:\n",
    "                    score = scorer.score(output_tm)\n",
    "                    results[name] = float(score)\n",
    "\n",
    "            # Print results\n",
    "            if self.verbose:\n",
    "                print(\"Results\")\n",
    "                print(\"============\")\n",
    "                for metric, score in results.items():\n",
    "                    print(f\"{metric}: {str(score)}\")\n",
    "                print(\" \")\n",
    "\n",
    "        return results\n",
    "\n",
    "def get_metrics(self):\n",
    "        \"\"\"Prepare evaluation measures using OCTIS\"\"\"\n",
    "        npmi = Coherence(texts=self.data.get_corpus(), topk=self.topk, measure=\"c_npmi\")\n",
    "        topic_diversity = TopicDiversity(topk=self.topk)\n",
    "\n",
    "        # Define methods\n",
    "        coherence = [(npmi, \"npmi\")]\n",
    "        diversity = [(topic_diversity, \"diversity\")]\n",
    "        metrics = [(coherence, \"Coherence\"), (diversity, \"Diversity\")]\n",
    "\n",
    "        return metrics\n",
    "# 1) 토큰화 (이미 있는 함수)\n",
    "corpus_tokens = corpus_to_tokens(texts)\n",
    "\n",
    "# 2) 실행부 전용 정제 (빈 문서 제거 + vocab 필터 + 길이>=2 토픽만 유지)\n",
    "def sanitize_for_coherence_exec_side(corpus_tokens, topics, max_topk=10):\n",
    "    nonempty_docs = [doc for doc in corpus_tokens if len(doc) > 0]\n",
    "    vocab = set(w for doc in nonempty_docs for w in doc)\n",
    "\n",
    "    topics_clean = []\n",
    "    for t in topics:\n",
    "        tt = [w for w in t if w in vocab]\n",
    "        if len(tt) >= 2:\n",
    "            topics_clean.append(tt)\n",
    "\n",
    "    if not nonempty_docs:\n",
    "        raise ValueError(\"All documents became empty after tokenization.\")\n",
    "    if not topics_clean:\n",
    "        raise ValueError(\"All topics became <2 words after filtering.\")\n",
    "\n",
    "    # 핵심: 모든 토픽 길이를 만족하는 safe topk 산출\n",
    "    min_len = min(len(t) for t in topics_clean)\n",
    "    effective_topk = min(max_topk, min_len)\n",
    "\n",
    "    # 각 토픽을 effective_topk로 슬라이스 (OCTIS가 topk 상위만 사용)\n",
    "    topics_trimmed = [t[:effective_topk] for t in topics_clean]\n",
    "\n",
    "    print(f\"[info] docs={len(nonempty_docs)} | topics_in={len(topics)} \"\n",
    "          f\"| topics_used={len(topics_trimmed)} | min_topic_len={min_len} \"\n",
    "          f\"| topk={effective_topk}\")\n",
    "\n",
    "    return nonempty_docs, topics_trimmed, effective_topk\n",
    "\n",
    "corpus_tokens_clean, topics_clean, effective_topk = sanitize_for_coherence_exec_side(\n",
    "    corpus_tokens, topics, max_topk=10\n",
    ")\n",
    "\n",
    "# 3) Dummy 객체는 수정 없이, 다만 topk를 safe 값으로 전달\n",
    "class Dummy:\n",
    "    def __init__(self, corpus_tokens, verbose=True, topk=10):\n",
    "        self.data = self\n",
    "        self._corpus = corpus_tokens\n",
    "        self.verbose = verbose\n",
    "        self.topk = topk\n",
    "        self.timestamps = None\n",
    "        self.metrics = self.get_metrics()\n",
    "\n",
    "    def get_corpus(self):\n",
    "        return self._corpus\n",
    "\n",
    "    evaluate = evaluate\n",
    "    get_metrics = get_metrics\n",
    "\n",
    "dummy = Dummy(corpus_tokens_clean, verbose=True, topk=effective_topk)\n",
    "\n",
    "# 4) OCTIS는 dict 입력 필요\n",
    "results = dummy.evaluate({\"topics\": topics_clean})\n",
    "print(\"Final results:\", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ys0660",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
